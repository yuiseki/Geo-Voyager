import { Question } from "@prisma/client";
import { prisma } from "../db";
import {
  getAllRejectedHypothesesByQuestionId,
  getAllHypothesesByStatus,
  HypothesisStatus,
  getAllOtherHypothesesByQuestionId,
} from "../db/hypothesis";
import { ChatOllama } from "@langchain/ollama";

/**
 * æ–°ã—ã„ä»®èª¬ã‚’ç”Ÿæˆã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
 * @param question é–¢é€£ã™ã‚‹Question
 * @returns ç”Ÿæˆã•ã‚ŒãŸæ–°ã—ã„Hypothesis
 */
export const formulateNewHypothesis = async (question: Question) => {
  console.log("ğŸ§  Formulating a new hypothesis...");

  // æ–°ã—ã„ä»®èª¬ã‚’ç”Ÿæˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆä»®èª¬ã®ç«‹æ¡ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
  const hypothesisDescription = await formulateNewHypothesisFromQuestion(
    question
  );

  if (!hypothesisDescription) {
    console.error("âš ï¸  Failed to formulate a new hypothesis.");
    return null;
  }

  // æ–°ã—ã„ä»®èª¬ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
  const newHypothesis = await prisma.hypothesis.create({
    data: {
      description: hypothesisDescription,
      status: HypothesisStatus.PENDING,
      questionId: question.id,
    },
  });

  console.log(`ğŸ’¡ New Hypothesis: ${newHypothesis.description}`);
  return newHypothesis;
};

/**
 * è³ªå•ã®èª¬æ˜ã‚’å…ƒã«æ–°ã—ã„ä»®èª¬ã‚’ç”Ÿæˆ
 * @param question Question
 * @returns ç”Ÿæˆã•ã‚ŒãŸæ–°ã—ã„ä»®èª¬
 */
const formulateNewHypothesisFromQuestion = async (
  question: Question
): Promise<string> => {
  const exampleHypotheses = await getAllOtherHypothesesByQuestionId(
    question.id
  );
  const rejectedHypotheses = await getAllRejectedHypothesesByQuestionId(
    question.id
  );

  // ä»®èª¬ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
  const prompt = `Given the question: "${
    question.description
  }", formulate a new testable hypothesis in Japanese.

Examples of testable hypotheses for other questions:
${exampleHypotheses.map((h) => `- ${h.description}`).join("\n")}

Already rejected hypotheses for this question:
${rejectedHypotheses.map((h) => `- ${h.description}`).join("\n")}

Reply only with the hypothesis description.`;

  // console.log("ğŸ¤– Hypothesis generation prompt:");
  // console.log(prompt);

  // AIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¨ã—ã¦æ–°ã—ã„ä»®èª¬ã‚’ç”Ÿæˆ
  const model = new ChatOllama({
    model: "qwen2.5:7b",
    temperature: 0,
  });
  const hypothesisDescription = await model.invoke(prompt);

  return hypothesisDescription.content as string;
};
